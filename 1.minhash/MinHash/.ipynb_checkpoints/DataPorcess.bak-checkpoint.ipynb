{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import binascii\n",
    "import pprint\n",
    "import nltk\n",
    "from bisect import bisect_right\n",
    "from heapq import heappop, heappush\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numHashes = 10\n",
    "numDocs = 100\n",
    "dataFile = \"./data/articles_\" + str(numDocs) + \".train\"\n",
    "truthFile = \"./data/articles_\" + str(numDocs) + \".truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plagiaries = {}\n",
    "\n",
    "# Open the truth file.\n",
    "f = open(truthFile, \"rU\")\n",
    "\n",
    "# For each line of the files...\n",
    "for line in f:\n",
    "  # Strip the newline character, if present.\n",
    "  if line[-1] == '\\n':\n",
    "      line = line[0:-1]\n",
    "      \n",
    "  docs = line.split(\" \")\n",
    "\n",
    "  # Map the two documents to each other.\n",
    "  plagiaries[docs[0]] = docs[1]\n",
    "  plagiaries[docs[1]] = docs[0]\n",
    "\n",
    "# pprint.pprint(plagiaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(dataFile, \"rU\")\n",
    "\n",
    "for i in range(0, 1):\n",
    "    # each line is an article, words[0] is article id\n",
    "    line = f.readline()\n",
    "    id_article = line.split(' ', 1)\n",
    "    \n",
    "    singles = set()\n",
    "    \n",
    "#     # Words tokenize\n",
    "#     tokenize = nltk.word_tokenize(id_article[1])\n",
    "#     ngram_gen = ngrams(tokenize,5)\n",
    "#     for grams in ngram_gen:\n",
    "#         single = ' '.join(grams)\n",
    "#         singles.add(single)\n",
    "#         # print single\n",
    "    \n",
    "    # Characters tokenize\n",
    "    tokenize = list(id_article[1])\n",
    "    ngram_gen = ngrams(tokenize,26)\n",
    "    for grams in ngram_gen:\n",
    "        single = ''.join(grams)\n",
    "        singles.add(single)\n",
    "        # print single\n",
    "\n",
    "pprint.pprint(singles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Shingling articles...\"\n",
    "\n",
    "# The current shingle ID value to assign to the next new shingle we \n",
    "# encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "# value.\n",
    "curShingleID = 0\n",
    "\n",
    "# Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "# \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "docsAsShingleSets = {};\n",
    "  \n",
    "# Open the data file.\n",
    "f = open(dataFile, \"rU\")\n",
    "\n",
    "docNames = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "totalShingles = 0\n",
    "\n",
    "for i in range(0, numDocs):\n",
    "  \n",
    "  # Read all of the words (they are all on one line) and split them by white\n",
    "  # space.\n",
    "  words = f.readline().split(\" \") \n",
    "  \n",
    "  # Retrieve the article ID, which is the first word on the line.  \n",
    "  docID = words[0]\n",
    "  \n",
    "  # Maintain a list of all document IDs.  \n",
    "  docNames.append(docID)\n",
    "    \n",
    "  del words[0]  \n",
    "  \n",
    "  # 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
    "  # current document. If a shingle ID occurs multiple times in the document,\n",
    "  # it will only appear once in the set (this is a property of Python sets).\n",
    "  shinglesInDoc = set()\n",
    "  \n",
    "  # For each word in the document...\n",
    "  for index in range(0, len(words) - 2):\n",
    "    # Construct the shingle text by combining three words together.\n",
    "    shingle = words[index] + \" \" + words[index + 1] + \" \" + words[index + 2]\n",
    "\n",
    "    # Hash the shingle to a 32-bit integer.\n",
    "    crc = binascii.crc32(shingle) & 0xffffffff\n",
    "    \n",
    "    # Add the hash value to the list of shingles for the current document. \n",
    "    # Note that set objects will only add the value to the set if the set \n",
    "    # doesn't already contain it. \n",
    "    shinglesInDoc.add(crc)\n",
    "  \n",
    "  # Store the completed list of shingles for this document in the dictionary.\n",
    "  docsAsShingleSets[docID] = shinglesInDoc\n",
    "  \n",
    "  # Count the number of shingles across all documents.\n",
    "  totalShingles = totalShingles + (len(words) - 2)\n",
    "\n",
    "# Close the data file.  \n",
    "f.close()  \n",
    "\n",
    "# Report how long shingling took.\n",
    "print '\\nShingling ' + str(numDocs) + ' docs took %.2f sec.' % (time.time() - t0)\n",
    " \n",
    "print '\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
